(m)Â A computational [threshold of 10^26 floating point operations]{According to [Epoch AI's estimates](https://epoch.ai/data/notable-ai-models), no company had ever published a foundational model above this threshold until early 2025. Since then, only two companies (OpenAI and xAI) have released models that pass the threshold.\\
President Biden's [Executive Order 14110](https://www.govinfo.gov/content/pkg/FR-2023-11-01/pdf/2023-24283.pdf) also used 10^26 training FLOPs as the threshold for a frontier model. Under EO 14110, any developer training a model above the threshold was required to disclose their safety testing results and security protocols to Federal authorities.} captures the current frontier of foundation model development and captures only highly resourced developers spending [hundreds of millions of dollars]{This estimate is credible. A 10^26 FLOP training run on NVIDIA H100s at 50% utilization would require high tens of millions of GPU hours, which would cost over one hundred million dollars at [current prices](https://cloud.google.com/compute/gpus-pricing).\\
For comparison, New York's proposed [RAISE Act](https://www.nysenate.gov/legislation/bills/2025/A6453/amendment/A) counts any person who has spent over five million dollars on a single training run as a large developer. SB 53's definition of a large developer is thus far more conservative than the RAISE Act's threshold, and far less likely to burden small developers with unnecessary regulation.} to develop foundation models.

Unlike the [RAISE Act](https://www.nysenate.gov/legislation/bills/2025/A6453/amendment/A) recently approved by the New York state legislature, SB 53 would *not* require large developers to conduct regular third-party audits of their safety and security practices. But this paragraph does require developers to report whether and how they are conducting third-party audits or assessments.\\