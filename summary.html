<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SB 53 in brief</title>
    <link rel="icon" type="image/png" href="/53favicon.png">
    <style>
        body {
            font-family: Georgia, serif;
            line-height: 1.8;
            margin: 0;
            padding: 20px;
            background-color: #fafafa;
            color: #333;
        }
        
        .container {
            max-width: 700px;
            margin: 0 auto;
            background: white;
            padding: 3rem;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        h1 {
            text-align: center;
            color: #333;
            margin-bottom: 2rem;
        }
        
        h2 {
            color: #333;
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-size: 1.3em;
        }
        
        h3 {
            color: #555;
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
            font-size: 1.1em;
        }
        
        h4 {
            color: #666;
            margin-top: 1rem;
            margin-bottom: 0.5rem;
            font-size: 1em;
        }
        
        p {
            margin-bottom: 1rem;
        }
        
        a {
            color: #1976d2;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        /* List styling */
        ul, ol {
            margin: 1rem 0;
            padding-left: 2rem;
        }
        
        li {
            margin-bottom: 0.5rem;
        }
        
        /* Footnote styling */
        .footnote-ref {
            font-weight: bold;
            padding: 0 2px;
        }
        
        .footnotes {
            margin-top: 3rem;
            font-size: 0.9em;
        }
        
        .footnotes hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin-bottom: 1rem;
        }
        
        .footnotes ol {
            padding-left: 1.5rem;
        }
        
        .footnotes li {
            margin-bottom: 0.5rem;
            color: #666;
        }
        
        .footnote-backref {
            margin-left: 0.5rem;
            font-size: 0.85em;
        }
        
        /* Navigation */
        .nav-bar {
            text-align: center;
            margin-bottom: 2rem;
            padding-bottom: 1rem;
            border-bottom: 1px solid #e0e0e0;
        }
        
        .nav-link {
            display: inline-block;
            margin: 0 1rem;
            color: #666;
            font-size: 0.9em;
        }
        
        .nav-link:hover {
            color: #1976d2;
        }
        
        .nav-link.back {
            font-weight: bold;
        }
        
        /* Emphasis */
        strong {
            color: #333;
        }
        
        em {
            font-style: italic;
        }
        
        /* Mobile responsive */
        @media (max-width: 600px) {
            .container {
                padding: 2rem 1.5rem;
            }
            
            h1 {
                font-size: 1.5em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <nav class="nav-bar">
            <a href="index" class="nav-link back">← Back to Bill Text</a>
            <a href="summary" class="nav-link">Summary</a>
            <a href="faq" class="nav-link">FAQs</a>
        </nav>
        
        <h1>SB 53 in brief</h1>
        
        On a high level, SB 53 says seven things. These are the four I consider most important:
<ul>
<li>Every large AI developer must write, publish, and follow a safety policy (§ 22757.12.a). </li>
<li>Every year starting in 2030, a large AI developer must get an independent auditor to verify that (1) they are following their own safety policy, and (2) the safety policy is clear enough that it's possible to determine whether the developer is following it (§ 22757.14).</li>
<li>Every frontier model released by a large AI developer must have a published model card (§ 22757.12.c)</li>
<li>A large AI developer is civilly liable if they break any of these rules (§ 22757.16). </li>
</ul>

And these are the three major provisions that I expect will be less important:
<ul>
<li>The California Attorney General will operate an incident reporting system for critical incidents involving AI (§ 22757.13).</li>
<li>Whistleblower protections for large AI developers' employees and external partners are expanded (§ 1107).</li>
<li>California will explore building a public AI compute cluster to support socially beneficial AI research and innovation (§ 11546.8).</li>
</ul>

<p>Large AI developers are not currently mandated to adopt <strong>safety policies</strong>, but under SB 53, they would be. This wouldn't require most frontier developers to do anything qualitatively new, since they already have published safety policies, and they've already made non-enforceable commitments to follow those policies. <a href="https://www-cdn.anthropic.com/f3b282f157017d08e36636bda1bf3bd4d9f23ee7.pdf">Anthropic</a>, <a href="https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf">OpenAI</a>, <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/updating-the-frontier-safety-framework/Frontier%20Safety%20Framework%202.0.pdf">Google DeepMind</a>, and <a href="https://ai.meta.com/static-resource/meta-frontier-ai-framework/?utm<em>source=newsroom&amp;utm</em>medium=web&amp;utm<em>content=Frontier</em>AI<em>Framework</em>PDF">Meta</a> have all written safety policies that satisfy many of the requirements in § 22757.12.a, and <a href="https://x.ai/documents/2025.02.20-RMF-Draft.pdf">xAI</a> has a draft safety policy that satisfies a few of the requirements. So if SB 53 were to pass, one industry laggard would have to write a safety policy for the first time, other frontier developers would have to make their existing safety policies more robust, and every frontier developer would be legally mandated to follow their own safety policy.</p>

<p>Moreover, they must get an <strong>independent auditor</strong> to certify that they are following their safety policy, and this explicitly includes certifying that the safety policy is <em>clear enough</em> for it to be determinate whether the developer is complying with it. As far as is publicly known, no major AI developer has ever undergone a safety audit, but such audits are completely routine in other risky industries <a href="https://www.ecfr.gov/current/title-14/chapter-I/subchapter-G/part-121?toc=1">like aviation</a>. Every airline in the US is required to write a plan explaining what measures they will follow to ensure safety, and they must regularly commission independent audits to confirm that they're following the plan. SB 53 would require companies developing frontier AI systems to do the same.</p>

<p>It is already a widely accepted best practice in the AI industry that when a company releases a new frontier model, they should publish a report called a <a href="https://arxiv.org/abs/1810.03993"><strong>model card</strong></a> describing the model's capabilities to consumers and to the scientific community. Anthropic, OpenAI, and Google DeepMind have consistently released model cards alongside all of their recent frontier models, and all three companies' model cards likely comply with most of the requirements in SB 53. These cards generally explain how the developer assessed the risks posed by their model, how they intend to mitigate those risks, and whether their model reached any prespecified risk or capability thresholds. If the bill were to pass, the big three AI developers would have to disclose more detailed information about third party assessments run on their models, and developers like xAI that generally don't publish model cards would have to start publishing them.</p>

<p>SB 53 would make large developers <strong>civilly liable</strong> for breaches of the above rules. No AI company executives will go to jail for failing to publish a safety policy or model card, but their companies can be faced with heavy fines—up to millions of dollars for a knowing violation of the law that causes material catastrophic risk. This is a major change from the <em>status quo</em>. Today, frontier AI developers have no legal obligation to disclose anything about their safety and security protocols to government, let alone to the public. When a company releases a new AI system more powerful than any system before, it is entirely optional under present law for them to tell consumers what dangerous things that system can do. And if a company does choose to adopt a safety policy or publish a model card, there is no force of law to guarantee the safety policy is being implemented or that the model card is accurate. This would all change under SB 53. We'd no longer have to rely on AI developers' good will to share critical safety information with the public.</p>

<p>There is currently no official channel for the California state government to <strong>collect reports of safety incidents</strong> involving AI. If a frontier AI developer discovered tomorrow that the weights of their leading model had been stolen, the best they could do to alert state authorities would probably be to email the Attorney General's office. If a member of the public witnessed an AI autonomously causing harm in the wild, the fastest way for them to tell the authorities would probably be to tweet about it. SB 53 would replace these slow, informal information channels with an official incident reporting mechanism run by the AG. Just like California has an <a href="https://oag.ca.gov/privacy/databreach/reporting">official website to collect</a> reports of data breaches, there would be another site for reports of critical AI safety incidents.</p>

<p><a href="https://leginfo.legislature.ca.gov/faces/codes_displaySection.xhtml?lawCode=LAB&amp;sectionNum=1102.5">Existing California law</a> already offers <strong>whistleblower protection</strong> to AI company employees who report a violation of federal, state, or local law to public officials or to their superiors. Companies may not make rules or enforce contracts that would prevent their employees from blowing the whistle, nor can they retaliate against an employee who becomes a whistleblower. SB 53 expands the scope of these protections in two ways. First, it would grant whistleblower protection to actors who are currently not protected. Independent contractors, freelancers, unpaid advisors, and external groups that help developers to assess and manage catastrophic risk are not protected by existing law if they become whistleblowers, but they would be under SB 53. Second, the bill would protect disclosures of evidence that an AI developer's activities pose a catastrophic risk, whereas existing law only protects disclosures of evidence that a developer is breaking the law. Of course, many ways that a developer could cause a catastrophic risk would also involve breaking the law, but it's conceivable that a developer could do something catastrophically dangerous yet legal. It might also be easier for many would-be whistleblowers to tell whether their employer is causing a catastrophic risk than to tell whether their employer is breaking a specific law.</p>

<p>Finally, SB 53 calls for California to build a <strong>publicly owned AI compute</strong> cluster called CalCompute. The cluster's purpose would be to support AI research and innovation for the public benefit. Nothing like CalCompute currently exists in California, but similar projects have been announced or are already underway in several other jurisdictions. New York has already built a compute cluster under their <a href="https://www.empireai.edu/">Empire AI</a> initiative, the UK has given academics compute access through its <a href="https://www.ukri.org/news/300-million-to-launch-first-phase-of-new-ai-research-resource">AI Research Resource</a>, and the US National Science Foundation's <a href="https://nairrpilot.org/">National AI Research Resource</a> aims to provide the same for American researchers. SB 53 does not specify how much funding California will put behind CalCompute, nor how many AI chips it aims to acquire, so it's hard to tell how much this section of the bill will accomplish. If CalCompute is funded generously in the next state budget, it could be a big deal, but if the project only gets a meager budget, it may not achieve much.</p>
        
        
    </div>
</body>
</html>