<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SB 53 in brief</title>
    <link rel="icon" type="image/png" href="/53favicon.png">
    <style>
        body {
            font-family: Georgia, serif;
            line-height: 1.8;
            margin: 0;
            padding: 20px;
            background-color: #fafafa;
            color: #333;
        }
        
        .container {
            max-width: 700px;
            margin: 0 auto;
            background: white;
            padding: 3rem;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        h1 {
            text-align: center;
            color: #333;
            margin-bottom: 2rem;
        }
        
        h2 {
            color: #333;
            margin-top: 2rem;
            margin-bottom: 1rem;
            font-size: 1.3em;
        }
        
        h3 {
            color: #555;
            margin-top: 1.5rem;
            margin-bottom: 0.75rem;
            font-size: 1.1em;
        }
        
        h4 {
            color: #666;
            margin-top: 1rem;
            margin-bottom: 0.5rem;
            font-size: 1em;
        }
        
        p {
            margin-bottom: 1rem;
        }
        
        a {
            color: #1976d2;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        /* List styling */
        ul, ol {
            margin: 1rem 0;
            padding-left: 2rem;
        }
        
        li {
            margin-bottom: 0.5rem;
        }
        
        /* Footnote styling */
        .footnote-ref {
            font-weight: bold;
            padding: 0 2px;
        }
        
        .footnotes {
            margin-top: 3rem;
            font-size: 0.9em;
        }
        
        .footnotes hr {
            border: none;
            border-top: 1px solid #e0e0e0;
            margin-bottom: 1rem;
        }
        
        .footnotes ol {
            padding-left: 1.5rem;
        }
        
        .footnotes li {
            margin-bottom: 0.5rem;
            color: #666;
        }
        
        .footnote-backref {
            margin-left: 0.5rem;
            font-size: 0.85em;
        }
        
        /* Navigation */
        .nav-bar {
            text-align: center;
            margin-bottom: 2rem;
            padding-bottom: 1rem;
            border-bottom: 1px solid #e0e0e0;
        }
        
        .nav-link {
            display: inline-block;
            margin: 0 1rem;
            color: #666;
            font-size: 0.9em;
        }
        
        .nav-link:hover {
            color: #1976d2;
        }
        
        .nav-link.back {
            font-weight: bold;
        }
        
        /* Emphasis */
        strong {
            color: #333;
        }
        
        em {
            font-style: italic;
        }
        
        /* Mobile responsive */
        @media (max-width: 600px) {
            .container {
                padding: 2rem 1.5rem;
            }
            
            h1 {
                font-size: 1.5em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <nav class="nav-bar">
            <a href="index" class="nav-link back">← Back to Bill Text</a>
            <a href="summary" class="nav-link">Summary</a>
            <a href="faq" class="nav-link">FAQs</a>
        </nav>
        
        <h1>SB 53 in brief</h1>
        
        On a high level, SB 53 says seven things:
<ul>
<li>Every large frontier AI developer must write, publish, and follow a frontier AI framework, which is similar to a <a href="https://metr.org/faisc">frontier safety policy</a> (§ 22757.12.a). </li>
<li>Every time a large frontier AI developer deploys a new frontier model, they must publish a transparency report explaining how they assessed catastrophic risks from their model, what they found, and how third parties were involved (§ 22757.12.c).</li>
<li>Every three months, a large frontier developer must assess catastrophic risk from their internal use of frontier models and share their assessment with the California <a href="https://www.caloes.ca.gov/">Office of Emergency Services</a> (§ 22757.12.d).</li>
<li>A large AI developer is civilly liable for a fine of up to one million dollars if they break any of these rules (§ 22757.15). </li>
<li>The California Office of Emergency Services will operate an incident reporting system for critical incidents involving AI (§ 22757.13).</li>
<li>Employees of frontier AI developers who are responsible for risk assessment or management get strengthened whistleblower protections (§ 1107).</li>
<li>California will explore building a public AI compute cluster to support socially beneficial AI research and innovation (§ 11546.8).</li>
</ul>

<p>SB 53 is the first American law mandating frontier AI developers to adopt <strong>safety policies</strong>, industry-standard tools for disclosing how a developer manages severe risks from its models. Most frontier developers including <a href="https://www-cdn.anthropic.com/f3b282f157017d08e36636bda1bf3bd4d9f23ee7.pdf">Anthropic</a>, <a href="https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf">OpenAI</a>, <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/strengthening-our-frontier-safety-framework/frontier-safety-framework<em>3.pdf">Google DeepMind</a>, <a href="https://data.x.ai/2025-08-20-xai-risk-management-framework.pdf">xAI</a>, and <a href="https://ai.meta.com/static-resource/meta-frontier-ai-framework/">Meta</a> already have safety policies that satisfy many of SB 53's requirements. The new law requires these frontier developers to strengthen their safety policies in some key respects. For example, they will have to be more transparent about how they manage catastrophic risks from internal use of their models. SB 53 will also ensure that new developers reaching the frontier in the future publish comprehensive safety policies.</p>

<p>It is already a widely accepted best practice in the AI industry that when a company releases a new frontier model, they should publish a report called a <a href="https://arxiv.org/abs/1810.03993">model card</a> describing the model's capabilities and safety properties. SB 53 requires frontier AI companies to publish basic model cards, and it further requires that <em>large</em> frontier AI companies <strong>publish safety information in a transparency report</strong> for every new frontier model they deploy. The report must include accounts of how the developer assessed catastrophic risks from their model, what results they found, how third parties were involved, and what else they did to comply with their own safety policy.</p>

<p>Frontier AI frameworks and transparency reports aren't qualitatively new. Leading AI companies were already publishing them before SB 53 passed. But the law does require one new kind of disclosure. Large frontier developers must regularly <strong>assess catastrophic risks from internal use of their models</strong> and report what they find to the Office of Emergency Services. In contrast, AI companies' past catastrophic risk assessments have generally focused on risks from exernal deployment, and they have been tied to new model releases instead of occurring on a regular schedule.</p>

<p>SB 53 makes large developers <strong>civilly liable</strong> for breaches of the above rules. No AI company executives will go to jail for failing to publish a safety policy or model card, but their companies can be faced with fines—up to one million dollars per violation. This is a major change from the previous <em>status quo</em>. Frontier AI developers used to have no legal obligation to disclose anything about their safety and security protocols to government, let alone to the public. But now under SB 53, basic transparency about risk management is no longer optional, and there is force of law holding developers to their published safety commitments.</p>

<p>SB 53 creates an official channel for the California state government to <strong>collect reports of safety incidents</strong> involving AI. In order for the state government to mount an effective response to an ongoing safety incident, they need to be aware that the incident is unfolding. But many possible incidents would likely be noticed by AI developers or by members of the public before they were noticed by officials. SB 53 ensures that if this should happen, there will be an efficient mechanism for reports of a critical incident to reach the authorities. Further, it mandates that when a frontier developer becomes aware of an AI incident posing an imminent risk of death or serious injury, they must notify authorities within 24 hours.</p>

<a href="https://leginfo.legislature.ca.gov/faces/codes</em>displaySection.xhtml?lawCode=LAB&amp;sectionNum=1102.5">Existing California law</a> already offers <strong>whistleblower protection</strong> to AI company employees who report a violation of federal, state, or local law to public officials or to their superiors. Companies may not make rules or enforce contracts that would prevent their employees from blowing the whistle, nor can they retaliate against an employee who becomes a whistleblower. SB 53 strengthens the whistleblower rights of employees on frontier AI developers' safety teams in two ways. 
<ul>
<li>First, it protects disclosures by these employees of evidence that an AI developer's activities pose a catastrophic risk, whereas existing law only protects disclosures of evidence that a developer is breaking the law. Of course, many ways that a developer could cause a catastrophic risk would also involve breaking the law, but it's conceivable that a developer could do something catastrophically dangerous yet legal. It might also be easier for many would-be whistleblowers to tell whether their employer is causing a catastrophic risk than to tell whether their employer is breaking a specific law.</li>
<li>Second, SB 53 introduces a new requirement for frontier AI developers to notify risk management staff of their whistleblower rights in writing every year and to obtain acknowledgement from all of these staff that they have read their rights.</li>
</ul>

<p>Finally, SB 53 calls for California to build a <strong>publicly owned AI compute</strong> cluster called CalCompute. The cluster's purpose would be to support AI research and innovation for the public benefit. Nothing like CalCompute currently exists in California, but similar projects have been announced or are already underway in several other jurisdictions. New York has already built a compute cluster under their <a href="https://www.empireai.edu/">Empire AI</a> initiative, the UK has given academics compute access through its <a href="https://www.ukri.org/news/300-million-to-launch-first-phase-of-new-ai-research-resource">AI Research Resource</a>, and the US National Science Foundation's <a href="https://nairrpilot.org/">National AI Research Resource</a> aims to provide the same for American researchers. SB 53 does not specify how much funding California will put behind CalCompute, nor how many AI chips it aims to acquire, so it's hard to tell how much this section of the bill will accomplish. If CalCompute is funded generously in the next state budget, it could be a big deal, but if the project only gets a meager budget, it may not achieve much.</p>
        
        
    </div>
</body>
</html>